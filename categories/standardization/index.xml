<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Standardization on Consortium for Python Data API Standards</title>
    <link>https://data-apis.org/categories/standardization/</link>
    <description>Recent content in Standardization on Consortium for Python Data API Standards</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Tue, 24 Aug 2021 00:00:00 +0000</lastBuildDate><atom:link href="https://data-apis.org/categories/standardization/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>First release of the Array API Standard</title>
      <link>https://data-apis.org/blog/array_api_standard_release/</link>
      <pubDate>Tue, 10 Nov 2020 08:00:00 +0000</pubDate>
      
      <guid>https://data-apis.org/blog/array_api_standard_release/</guid>
      <description>Array and tensor libraries - from NumPy, TensorFlow and PyTorch to Dask, JAX, MXNet and beyond - could benefit greatly from a uniform API for creating and working with multi-dimensional arrays (a.k.a tensors), as we discussed in our previous blog post. Today we&amp;rsquo;re pleased to announce a first version of our array API standard (document, repo) for review by the wider community. Getting to this point took slightly longer than we had initially announced because, well, it&amp;rsquo;s 2020 and hence nothing quite goes according to plan.</description>
    </item>
    
    <item>
      <title>Towards dataframe interoperability</title>
      <link>https://data-apis.org/blog/dataframe_protocol_rfc/</link>
      <pubDate>Tue, 24 Aug 2021 00:00:00 +0000</pubDate>
      
      <guid>https://data-apis.org/blog/dataframe_protocol_rfc/</guid>
      <description>In the PyData ecosystem we have a large number of dataframe libraries as of today, each with their own strengths and weaknesses. Pandas is the most popular library today. Other libraries offer significant capabilities beyond what it provides though - impressive performance gains for Vaex (CPU) and cuDF (GPU), distributed dataframes for Modin and Dask, or leveraging Spark as an execution engine for Koalas. For downstream library authors, it would be powerful to be able to work with all these libraries.</description>
    </item>
    
  </channel>
</rss>
